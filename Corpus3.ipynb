{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSxdDL02SVXltI9UPepRN7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuhri05-code/telegram_bot/blob/main/Corpus3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 scipy==1.11.4 gensim==4.3.2 --force-reinstall --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmTel7gtu7mI",
        "outputId": "e918b375-1503-45e3-94d6-c29a9c4a1340"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gensim (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZqjP3X6vsrZ",
        "outputId": "7dfed9c7-2811-48bb-96de-5731593779d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# Feature Extraction NLP (Google Colab)\n",
        "# =====================================\n",
        "\n",
        "# Install pustaka (Colab biasanya fresh)\n",
        "!pip install scikit-learn gensim nltk --quiet\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "\n",
        "# Download resource NLTK (punkt = tokenizer, stopwords optional)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# -------------------------------------\n",
        "# Dataset sederhana (corpus)\n",
        "# -------------------------------------\n",
        "corpus = [\n",
        "    \"Natural Language Processing is a subfield of Artificial Intelligence\",\n",
        "    \"Neural Networks are widely used in NLP tasks\",\n",
        "    \"Feature extraction is important in text analysis\"\n",
        "]\n",
        "\n",
        "# -------------------------------------\n",
        "# 1. Bag of Words (BoW)\n",
        "# -------------------------------------\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(corpus)\n",
        "\n",
        "print(\"=== Bag of Words ===\")\n",
        "print(vectorizer_bow.get_feature_names_out())\n",
        "print(X_bow.toarray())\n",
        "\n",
        "# -------------------------------------\n",
        "# 2. TF-IDF\n",
        "# -------------------------------------\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(corpus)\n",
        "\n",
        "print(\"\\n=== TF-IDF ===\")\n",
        "print(vectorizer_tfidf.get_feature_names_out())\n",
        "print(X_tfidf.toarray())\n",
        "\n",
        "# -------------------------------------\n",
        "# 3. Word Embeddings (Word2Vec)\n",
        "# -------------------------------------\n",
        "# Tokenisasi\n",
        "tokenized_corpus = [nltk.word_tokenize(doc.lower()) for doc in corpus]\n",
        "\n",
        "# Latih Word2Vec\n",
        "model_w2v = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=50,\n",
        "    window=3,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "print(\"\\n=== Word2Vec Embedding (contoh kata 'nlp') ===\")\n",
        "print(model_w2v.wv['nlp'])  # vektor representasi untuk kata 'nlp'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlznM18Uuklf",
        "outputId": "1bd6cb98-0775-41e0-bb5e-074d2af5d0c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bag of Words ===\n",
            "['analysis' 'are' 'artificial' 'extraction' 'feature' 'important' 'in'\n",
            " 'intelligence' 'is' 'language' 'natural' 'networks' 'neural' 'nlp' 'of'\n",
            " 'processing' 'subfield' 'tasks' 'text' 'used' 'widely']\n",
            "[[0 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1]\n",
            " [1 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]]\n",
            "\n",
            "=== TF-IDF ===\n",
            "['analysis' 'are' 'artificial' 'extraction' 'feature' 'important' 'in'\n",
            " 'intelligence' 'is' 'language' 'natural' 'networks' 'neural' 'nlp' 'of'\n",
            " 'processing' 'subfield' 'tasks' 'text' 'used' 'widely']\n",
            "[[0.         0.         0.36325471 0.         0.         0.\n",
            "  0.         0.36325471 0.27626457 0.36325471 0.36325471 0.\n",
            "  0.         0.         0.36325471 0.36325471 0.36325471 0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.36325471 0.         0.         0.         0.\n",
            "  0.27626457 0.         0.         0.         0.         0.36325471\n",
            "  0.36325471 0.36325471 0.         0.         0.         0.36325471\n",
            "  0.         0.36325471 0.36325471]\n",
            " [0.40301621 0.         0.         0.40301621 0.40301621 0.40301621\n",
            "  0.30650422 0.         0.30650422 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.40301621 0.         0.        ]]\n",
            "\n",
            "=== Word2Vec Embedding (contoh kata 'nlp') ===\n",
            "[-0.01915709  0.01788623  0.00833014  0.01846947  0.01328701  0.00584947\n",
            "  0.01960804 -0.00884928 -0.01360662  0.00845476  0.007458   -0.01132922\n",
            "  0.01940952 -0.00711661  0.01909881  0.00166945 -0.01267691 -0.00395423\n",
            " -0.01475411 -0.00595905  0.00208339  0.01896537  0.0187117  -0.01319175\n",
            "  0.0069503   0.00455114 -0.0049787  -0.01845834  0.00205425 -0.01633141\n",
            "  0.01264038 -0.01160016  0.01107088  0.01966745 -0.00032     0.00905699\n",
            " -0.0036188   0.01472152  0.00788019 -0.01802065 -0.00479701  0.00725754\n",
            " -0.00019914 -0.00240254 -0.00211088 -0.0033432   0.00120991  0.00833019\n",
            " -0.00850558 -0.00766724]\n"
          ]
        }
      ]
    }
  ]
}